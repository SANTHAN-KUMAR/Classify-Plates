{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":15282,"databundleVersionId":565187,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T18:14:33.674012Z","iopub.execute_input":"2024-12-04T18:14:33.674574Z","iopub.status.idle":"2024-12-04T18:14:33.984149Z","shell.execute_reply.started":"2024-12-04T18:14:33.674544Z","shell.execute_reply":"2024-12-04T18:14:33.983195Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/platesv2/sample_submission.csv\n/kaggle/input/platesv2/plates.zip\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import zipfile\n\nzip_path = \"/kaggle/input/platesv2/plates.zip\"\nextract_path = \"/kaggle/working/plates\"\n\n# Extract the zip file\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_path)\n\n# Verify extraction\nprint(\"Extracted files:\", os.listdir(extract_path))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T18:14:33.985923Z","iopub.execute_input":"2024-12-04T18:14:33.986441Z","iopub.status.idle":"2024-12-04T18:14:34.373343Z","shell.execute_reply.started":"2024-12-04T18:14:33.986400Z","shell.execute_reply":"2024-12-04T18:14:34.371326Z"}},"outputs":[{"name":"stdout","text":"Extracted files: ['__MACOSX', 'plates']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_dir = os.listdir(\"/kaggle/working/plates/plates/train\")\ntest_dir = os.listdir(\"/kaggle/working/plates/plates/test\")\n\ntrain_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:00:05.324529Z","iopub.execute_input":"2024-12-03T18:00:05.324784Z","iopub.status.idle":"2024-12-03T18:00:05.332722Z","shell.execute_reply.started":"2024-12-03T18:00:05.324758Z","shell.execute_reply":"2024-12-03T18:00:05.331851Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['cleaned', '.DS_Store', 'dirty']"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from torchvision import datasets, transforms\n\n# Define the correct path to the train folder\ntrain_dir = \"/kaggle/working/plates/plates/train\"\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),   # Resize all images\n    transforms.ToTensor()            # Convert to Tensor\n])\n\n# Load the training dataset\ntrain_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n\n# Print classes to verify\nprint(\"Classes:\", train_dataset.classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:00:05.334674Z","iopub.execute_input":"2024-12-03T18:00:05.334970Z","iopub.status.idle":"2024-12-03T18:00:09.481330Z","shell.execute_reply.started":"2024-12-03T18:00:05.334946Z","shell.execute_reply":"2024-12-03T18:00:09.480473Z"}},"outputs":[{"name":"stdout","text":"Classes: ['cleaned', 'dirty']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Create DataLoader for the training dataset\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n\n# Check some sample data\ndata_iter = iter(train_loader)\nimages, labels = next(data_iter)\nprint(\"Batch of images:\", images.shape)\nprint(\"Batch of labels:\", labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:00:09.482848Z","iopub.execute_input":"2024-12-03T18:00:09.483231Z","iopub.status.idle":"2024-12-03T18:00:09.584975Z","shell.execute_reply.started":"2024-12-03T18:00:09.483204Z","shell.execute_reply":"2024-12-03T18:00:09.584111Z"}},"outputs":[{"name":"stdout","text":"Batch of images: torch.Size([4, 3, 224, 224])\nBatch of labels: tensor([0, 0, 1, 0])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom torchvision import models\n\n# Load ResNet18 model\nmodel = models.resnet18(pretrained=True)\n\n# Modify the final layer for 2 classes (dirty and clean)\nnum_features = model.fc.in_features\nmodel.fc = torch.nn.Linear(num_features, 2)  # Output size = 2\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:00:09.586007Z","iopub.execute_input":"2024-12-03T18:00:09.586271Z","iopub.status.idle":"2024-12-03T18:00:10.548069Z","shell.execute_reply.started":"2024-12-03T18:00:09.586246Z","shell.execute_reply":"2024-12-03T18:00:10.547359Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 201MB/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch.optim as optim\n\n# Loss function\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\noptimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:00:42.093467Z","iopub.execute_input":"2024-12-03T18:00:42.093796Z","iopub.status.idle":"2024-12-03T18:00:42.100258Z","shell.execute_reply.started":"2024-12-03T18:00:42.093766Z","shell.execute_reply":"2024-12-03T18:00:42.099387Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Training loop\nepochs = 50\nfor epoch in range(epochs):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:01:09.932288Z","iopub.execute_input":"2024-12-03T18:01:09.932621Z","iopub.status.idle":"2024-12-03T18:01:22.398547Z","shell.execute_reply.started":"2024-12-03T18:01:09.932591Z","shell.execute_reply":"2024-12-03T18:01:22.397640Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50, Loss: 0.6352248787879944\nEpoch 2/50, Loss: 0.5107301563024521\nEpoch 3/50, Loss: 0.5132587671279907\nEpoch 4/50, Loss: 0.4414399921894073\nEpoch 5/50, Loss: 0.4324199497699738\nEpoch 6/50, Loss: 0.40569522976875305\nEpoch 7/50, Loss: 0.4118296250700951\nEpoch 8/50, Loss: 0.452118257433176\nEpoch 9/50, Loss: 0.31927633583545684\nEpoch 10/50, Loss: 0.3165430337190628\nEpoch 11/50, Loss: 0.33702687323093417\nEpoch 12/50, Loss: 0.3507377587258816\nEpoch 13/50, Loss: 0.3134429931640625\nEpoch 14/50, Loss: 0.34882262647151946\nEpoch 15/50, Loss: 0.32602500542998314\nEpoch 16/50, Loss: 0.34604691416025163\nEpoch 17/50, Loss: 0.2711860820651054\nEpoch 18/50, Loss: 0.2708060421049595\nEpoch 19/50, Loss: 0.3262872636318207\nEpoch 20/50, Loss: 0.22157092541456222\nEpoch 21/50, Loss: 0.21189157366752626\nEpoch 22/50, Loss: 0.23923047557473182\nEpoch 23/50, Loss: 0.2114149495959282\nEpoch 24/50, Loss: 0.2825522720813751\nEpoch 25/50, Loss: 0.31170803755521775\nEpoch 26/50, Loss: 0.5270017474889755\nEpoch 27/50, Loss: 0.2820572957396507\nEpoch 28/50, Loss: 0.4292137950658798\nEpoch 29/50, Loss: 0.3251569621264935\nEpoch 30/50, Loss: 0.16891842782497407\nEpoch 31/50, Loss: 0.33992794826626777\nEpoch 32/50, Loss: 0.22790351137518883\nEpoch 33/50, Loss: 0.24796035885810852\nEpoch 34/50, Loss: 0.3640894487500191\nEpoch 35/50, Loss: 0.36227173656225203\nEpoch 36/50, Loss: 0.1535164812579751\nEpoch 37/50, Loss: 0.31143072694540025\nEpoch 38/50, Loss: 0.2663122616708279\nEpoch 39/50, Loss: 0.20320394709706308\nEpoch 40/50, Loss: 0.3616890698671341\nEpoch 41/50, Loss: 0.19599779620766639\nEpoch 42/50, Loss: 0.365081874281168\nEpoch 43/50, Loss: 0.35979450941085817\nEpoch 44/50, Loss: 0.26682375073432923\nEpoch 45/50, Loss: 0.2109085388481617\nEpoch 46/50, Loss: 0.33444128558039665\nEpoch 47/50, Loss: 0.23099133744835854\nEpoch 48/50, Loss: 0.2211947947740555\nEpoch 49/50, Loss: 0.15268484726548195\nEpoch 50/50, Loss: 0.14510790556669234\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from torchvision import transforms\nfrom PIL import Image\nimport os\n\n# Define the test folder path\ntest_dir = \"/kaggle/working/plates/plates/test\"\n\n# Define transformations (same as train)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\n# Load test images manually\ntest_images = []\ntest_ids = []\n\nfor img_name in os.listdir(test_dir):\n    img_path = os.path.join(test_dir, img_name)\n    image = Image.open(img_path).convert(\"RGB\")  # Ensure 3 channels (RGB)\n    image = transform(image)  # Apply transformations\n    test_images.append(image)\n    test_ids.append(img_name.split('.')[0])  # Extract ID from filename\n\nprint(f\"Loaded {len(test_images)} test images.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:05:14.322711Z","iopub.execute_input":"2024-12-03T18:05:14.323086Z","iopub.status.idle":"2024-12-03T18:05:16.268529Z","shell.execute_reply.started":"2024-12-03T18:05:14.323046Z","shell.execute_reply":"2024-12-03T18:05:16.267607Z"}},"outputs":[{"name":"stdout","text":"Loaded 744 test images.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from torchvision import transforms\nfrom PIL import Image\nimport os\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport pandas as pd\n\n# Define the test folder path\ntest_dir = \"/kaggle/working/plates/plates/test\"\n\n# Define transformations (same as train)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\n# Load test images manually\ntest_images = []\ntest_ids = []\n\nfor img_name in os.listdir(test_dir):\n    img_path = os.path.join(test_dir, img_name)\n    \n    # Skip non-image files like .DS_Store\n    if img_name.startswith('.') or not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n        continue\n    \n    image = Image.open(img_path).convert(\"RGB\")  # Ensure 3 channels (RGB)\n    image = transform(image)  # Apply transformations\n    test_images.append(image)\n    test_ids.append(img_name.split('.')[0])  # Extract ID from filename\n\nprint(f\"Loaded {len(test_images)} test images.\")\n\n# Convert list to tensor\ntest_images = torch.stack(test_images)\n\n# Create a TensorDataset for test data\ntest_dataset = TensorDataset(test_images)\n\n# Create a DataLoader for batching (batch_size 8)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n# Set model to evaluation mode\nmodel.eval()\n\n# List to store predictions\npredictions = []\n\n# Inference loop for test dataset\nwith torch.no_grad():  # No need to track gradients during inference\n    for images in test_loader:\n        images = images[0].to(device)  # Images are wrapped in a tuple, so access images[0]\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        predictions.extend(preds.cpu().numpy())\n\n# Convert predictions to 'clean' or 'dirty' labels\npredicted_labels = ['clean' if pred == 0 else 'dirty' for pred in predictions]\n\n# Create the submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': test_ids,\n    'label': predicted_labels\n})\n\n# Save the predictions to CSV\nsubmission_df.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"Submission saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:10:09.633106Z","iopub.execute_input":"2024-12-03T18:10:09.633441Z","iopub.status.idle":"2024-12-03T18:10:12.901402Z","shell.execute_reply.started":"2024-12-03T18:10:09.633411Z","shell.execute_reply":"2024-12-03T18:10:12.900491Z"}},"outputs":[{"name":"stdout","text":"Loaded 744 test images.\nSubmission saved!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"############   ABOVE CODE ACCURACY IS ONLY 59  ###########","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:12:52.894543Z","iopub.execute_input":"2024-12-03T18:12:52.895228Z","iopub.status.idle":"2024-12-03T18:12:52.899092Z","shell.execute_reply.started":"2024-12-03T18:12:52.895193Z","shell.execute_reply":"2024-12-03T18:12:52.898048Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nfrom sklearn.metrics import classification_report\nfrom PIL import Image\nimport os\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:12:56.494041Z","iopub.execute_input":"2024-12-03T18:12:56.494373Z","iopub.status.idle":"2024-12-03T18:12:57.003672Z","shell.execute_reply.started":"2024-12-03T18:12:56.494345Z","shell.execute_reply":"2024-12-03T18:12:57.002973Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:13:07.888724Z","iopub.execute_input":"2024-12-03T18:13:07.889279Z","iopub.status.idle":"2024-12-03T18:13:07.893399Z","shell.execute_reply.started":"2024-12-03T18:13:07.889248Z","shell.execute_reply":"2024-12-03T18:13:07.892515Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),   # Data augmentation\n    transforms.RandomRotation(30),       # Data augmentation\n    transforms.RandomAffine(15),         # Data augmentation\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for pre-trained models\n])\n\n# Define transformations for test (no augmentation, only resizing)\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:13:13.619704Z","iopub.execute_input":"2024-12-03T18:13:13.620349Z","iopub.status.idle":"2024-12-03T18:13:13.626129Z","shell.execute_reply.started":"2024-12-03T18:13:13.620314Z","shell.execute_reply":"2024-12-03T18:13:13.625239Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Define train directory\ntrain_dir = \"/kaggle/working/plates/plates/train\"\n\n# Load training data\ntrain_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n\n# Split dataset into training and validation sets (80% training, 20% validation)\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n# DataLoader for training, validation, and test\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\n# Define test directory and test data\ntest_dir = \"/kaggle/working/plates/plates/test\"\ntest_images = []\ntest_ids = []\n\nfor img_name in os.listdir(test_dir):\n    img_path = os.path.join(test_dir, img_name)\n    try:\n        image = Image.open(img_path).convert(\"RGB\")\n        image = test_transform(image)  # Apply transformations\n        test_images.append(image)\n        test_ids.append(img_name.split('.')[0])  # Extract ID from filename\n    except:\n        continue\n\n# Create a DataLoader for test dataset\ntest_images_tensor = torch.stack(test_images)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:13:20.419291Z","iopub.execute_input":"2024-12-03T18:13:20.420121Z","iopub.status.idle":"2024-12-03T18:13:22.950584Z","shell.execute_reply.started":"2024-12-03T18:13:20.420089Z","shell.execute_reply":"2024-12-03T18:13:22.949609Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Load pre-trained ResNet18 model and modify the final layer for 2 classes\nmodel = models.resnet18(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 2)  # Adjust for 2 classes (clean, dirty)\n\n# Move model to device\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:13:31.779185Z","iopub.execute_input":"2024-12-03T18:13:31.779530Z","iopub.status.idle":"2024-12-03T18:13:32.038877Z","shell.execute_reply.started":"2024-12-03T18:13:31.779499Z","shell.execute_reply":"2024-12-03T18:13:32.037961Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Learning rate scheduler (decreases LR after 7 epochs)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:14:03.723710Z","iopub.execute_input":"2024-12-03T18:14:03.724410Z","iopub.status.idle":"2024-12-03T18:14:03.729058Z","shell.execute_reply.started":"2024-12-03T18:14:03.724376Z","shell.execute_reply":"2024-12-03T18:14:03.728103Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Function to train the model\ndef train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=10):\n    best_acc = 0.0  # To keep track of the best accuracy during training\n    for epoch in range(num_epochs):\n        model.train()  # Set the model to training mode\n        running_loss = 0.0\n        correct_preds = 0\n        total_preds = 0\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # Zero gradients, perform a backward pass, and update the weights\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # Calculate accuracy\n            _, preds = torch.max(outputs, 1)\n            correct_preds += torch.sum(preds == labels).item()\n            total_preds += labels.size(0)\n\n            running_loss += loss.item()\n\n        # Print statistics for each epoch\n        train_loss = running_loss / len(train_loader)\n        train_acc = correct_preds / total_preds\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n\n        # Validation phase\n        model.eval()  # Set the model to evaluation mode\n        val_correct_preds = 0\n        val_total_preds = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, preds = torch.max(outputs, 1)\n                val_correct_preds += torch.sum(preds == labels).item()\n                val_total_preds += labels.size(0)\n\n        val_acc = val_correct_preds / val_total_preds\n        print(f\"Validation Accuracy: {val_acc:.4f}\")\n\n        # Save the best model based on validation accuracy\n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n\n        scheduler.step()  # Step the learning rate scheduler\n\n    print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:14:09.994181Z","iopub.execute_input":"2024-12-03T18:14:09.995044Z","iopub.status.idle":"2024-12-03T18:14:10.003411Z","shell.execute_reply.started":"2024-12-03T18:14:09.994996Z","shell.execute_reply":"2024-12-03T18:14:10.002510Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Train the model\ntrain_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=100)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:14:42.784189Z","iopub.execute_input":"2024-12-03T18:14:42.784495Z","iopub.status.idle":"2024-12-03T18:15:08.911479Z","shell.execute_reply.started":"2024-12-03T18:14:42.784470Z","shell.execute_reply":"2024-12-03T18:15:08.910565Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100, Loss: 0.4124, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 2/100, Loss: 0.3377, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 3/100, Loss: 0.3791, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 4/100, Loss: 0.3419, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 5/100, Loss: 0.1942, Accuracy: 0.9688\nValidation Accuracy: 0.6250\nEpoch 6/100, Loss: 0.2656, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 7/100, Loss: 0.2738, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 8/100, Loss: 0.3552, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 9/100, Loss: 0.2258, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 10/100, Loss: 0.4306, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 11/100, Loss: 0.2797, Accuracy: 0.8125\nValidation Accuracy: 0.6250\nEpoch 12/100, Loss: 0.2837, Accuracy: 0.8125\nValidation Accuracy: 0.6250\nEpoch 13/100, Loss: 0.2313, Accuracy: 0.9688\nValidation Accuracy: 0.5000\nEpoch 14/100, Loss: 0.2501, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 15/100, Loss: 0.2353, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 16/100, Loss: 0.2348, Accuracy: 0.9688\nValidation Accuracy: 0.6250\nEpoch 17/100, Loss: 0.2060, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 18/100, Loss: 0.2548, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 19/100, Loss: 0.3357, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 20/100, Loss: 0.2551, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 21/100, Loss: 0.2440, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 22/100, Loss: 0.3126, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 23/100, Loss: 0.3697, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 24/100, Loss: 0.1898, Accuracy: 0.9375\nValidation Accuracy: 0.5000\nEpoch 25/100, Loss: 0.2615, Accuracy: 0.8750\nValidation Accuracy: 0.5000\nEpoch 26/100, Loss: 0.2511, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 27/100, Loss: 0.2299, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 28/100, Loss: 0.3349, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 29/100, Loss: 0.2987, Accuracy: 0.8125\nValidation Accuracy: 0.6250\nEpoch 30/100, Loss: 0.2439, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 31/100, Loss: 0.2530, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 32/100, Loss: 0.3007, Accuracy: 0.8125\nValidation Accuracy: 0.6250\nEpoch 33/100, Loss: 0.3122, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 34/100, Loss: 0.2846, Accuracy: 0.8125\nValidation Accuracy: 0.6250\nEpoch 35/100, Loss: 0.3226, Accuracy: 0.7812\nValidation Accuracy: 0.6250\nEpoch 36/100, Loss: 0.2149, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 37/100, Loss: 0.2699, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 38/100, Loss: 0.3836, Accuracy: 0.8125\nValidation Accuracy: 0.6250\nEpoch 39/100, Loss: 0.2330, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 40/100, Loss: 0.2137, Accuracy: 0.9688\nValidation Accuracy: 0.6250\nEpoch 41/100, Loss: 0.2553, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 42/100, Loss: 0.2567, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 43/100, Loss: 0.1863, Accuracy: 0.9688\nValidation Accuracy: 0.6250\nEpoch 44/100, Loss: 0.2365, Accuracy: 0.9375\nValidation Accuracy: 0.5000\nEpoch 45/100, Loss: 0.2216, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 46/100, Loss: 0.1966, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 47/100, Loss: 0.2692, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 48/100, Loss: 0.3102, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 49/100, Loss: 0.2518, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 50/100, Loss: 0.3308, Accuracy: 0.8125\nValidation Accuracy: 0.5000\nEpoch 51/100, Loss: 0.2057, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 52/100, Loss: 0.2203, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 53/100, Loss: 0.3639, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 54/100, Loss: 0.3516, Accuracy: 0.8125\nValidation Accuracy: 0.6250\nEpoch 55/100, Loss: 0.2053, Accuracy: 0.9375\nValidation Accuracy: 0.7500\nEpoch 56/100, Loss: 0.2511, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 57/100, Loss: 0.1998, Accuracy: 0.9688\nValidation Accuracy: 0.6250\nEpoch 58/100, Loss: 0.1971, Accuracy: 0.9688\nValidation Accuracy: 0.5000\nEpoch 59/100, Loss: 0.2755, Accuracy: 0.9688\nValidation Accuracy: 0.6250\nEpoch 60/100, Loss: 0.2609, Accuracy: 0.9062\nValidation Accuracy: 0.5000\nEpoch 61/100, Loss: 0.2182, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 62/100, Loss: 0.1893, Accuracy: 0.9688\nValidation Accuracy: 0.6250\nEpoch 63/100, Loss: 0.3377, Accuracy: 0.8125\nValidation Accuracy: 0.6250\nEpoch 64/100, Loss: 0.2221, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 65/100, Loss: 0.3301, Accuracy: 0.8125\nValidation Accuracy: 0.7500\nEpoch 66/100, Loss: 0.2564, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 67/100, Loss: 0.2293, Accuracy: 0.9375\nValidation Accuracy: 0.5000\nEpoch 68/100, Loss: 0.2526, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 69/100, Loss: 0.2324, Accuracy: 0.9688\nValidation Accuracy: 0.5000\nEpoch 70/100, Loss: 0.2458, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 71/100, Loss: 0.2191, Accuracy: 0.9062\nValidation Accuracy: 0.5000\nEpoch 72/100, Loss: 0.3120, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 73/100, Loss: 0.2164, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 74/100, Loss: 0.2016, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 75/100, Loss: 0.1855, Accuracy: 0.9688\nValidation Accuracy: 0.5000\nEpoch 76/100, Loss: 0.2510, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 77/100, Loss: 0.2228, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 78/100, Loss: 0.2343, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 79/100, Loss: 0.2090, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 80/100, Loss: 0.4396, Accuracy: 0.8125\nValidation Accuracy: 0.6250\nEpoch 81/100, Loss: 0.2144, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 82/100, Loss: 0.2209, Accuracy: 0.9375\nValidation Accuracy: 0.5000\nEpoch 83/100, Loss: 0.2156, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 84/100, Loss: 0.2138, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 85/100, Loss: 0.3888, Accuracy: 0.8125\nValidation Accuracy: 0.6250\nEpoch 86/100, Loss: 0.3471, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 87/100, Loss: 0.1914, Accuracy: 0.9688\nValidation Accuracy: 0.6250\nEpoch 88/100, Loss: 0.2515, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 89/100, Loss: 0.3471, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 90/100, Loss: 0.2691, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 91/100, Loss: 0.3355, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 92/100, Loss: 0.2718, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 93/100, Loss: 0.2426, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 94/100, Loss: 0.2661, Accuracy: 0.9062\nValidation Accuracy: 0.6250\nEpoch 95/100, Loss: 0.3079, Accuracy: 0.8438\nValidation Accuracy: 0.6250\nEpoch 96/100, Loss: 0.4304, Accuracy: 0.8125\nValidation Accuracy: 0.6250\nEpoch 97/100, Loss: 0.2472, Accuracy: 0.8750\nValidation Accuracy: 0.6250\nEpoch 98/100, Loss: 0.2374, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nEpoch 99/100, Loss: 0.1978, Accuracy: 1.0000\nValidation Accuracy: 0.6250\nEpoch 100/100, Loss: 0.2282, Accuracy: 0.9375\nValidation Accuracy: 0.6250\nBest Validation Accuracy: 0.7500\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Load the best model\nmodel.load_state_dict(torch.load('best_model.pth'))\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Inference on test images\ntest_preds = []\nwith torch.no_grad():\n    for image in test_images_tensor:\n        image = image.unsqueeze(0).to(device)  # Add batch dimension\n        output = model(image)\n        _, pred = torch.max(output, 1)\n        test_preds.append(pred.item())\n\n# Convert predictions to labels\npredicted_labels = ['clean' if pred == 0 else 'dirty' for pred in test_preds]\n\n# Create a submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': test_ids,\n    'label': predicted_labels\n})\n\n# Save the predictions to CSV\nsubmission_df.to_csv('/kaggle/working/submission_plates.csv', index=False)\nprint(\"Submission saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:15:51.269118Z","iopub.execute_input":"2024-12-03T18:15:51.269761Z","iopub.status.idle":"2024-12-03T18:15:53.658998Z","shell.execute_reply.started":"2024-12-03T18:15:51.269727Z","shell.execute_reply":"2024-12-03T18:15:53.658127Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/3064993140.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Submission saved!\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"###############  ABOVE CODE'S ACCURACY IS TOO WORST - 21 ###################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:18:29.560026Z","iopub.execute_input":"2024-12-03T18:18:29.560356Z","iopub.status.idle":"2024-12-03T18:18:29.564310Z","shell.execute_reply.started":"2024-12-03T18:18:29.560328Z","shell.execute_reply":"2024-12-03T18:18:29.563403Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#################################################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T18:14:24.098041Z","iopub.execute_input":"2024-12-04T18:14:24.098763Z","iopub.status.idle":"2024-12-04T18:14:24.115468Z","shell.execute_reply.started":"2024-12-04T18:14:24.098724Z","shell.execute_reply":"2024-12-04T18:14:24.114696Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets, transforms, models\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nimport pandas as pd\n\n# Define the correct path to the train folder\ntrain_dir = \"/kaggle/working/plates/plates/train\"\n\n# Define transformations for training (minimal augmentation and normalization)\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),     # Resize all images to 224x224\n    transforms.RandomHorizontalFlip(), # Random horizontal flip\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Load the training dataset\ntrain_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n\n# Create DataLoader for the training dataset\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# Load ResNet50 model (deeper network)\nmodel = models.resnet50(pretrained=True)\n\n# Unfreeze all layers to allow fine-tuning\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Modify the final fully connected layer to match the number of classes\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)  # Output size = 2 (clean, dirty)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Optimizer for fine-tuning all layers\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# Loss function\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Learning rate scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n# Training loop\nepochs = 50\nbest_loss = float('inf')\npatience = 5\ncounter = 0\n\nfor epoch in range(epochs):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    # Learning rate adjustment\n    scheduler.step()\n\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n    \n    # Early stopping logic\n    if running_loss < best_loss:\n        best_loss = running_loss\n        counter = 0  # Reset counter\n        torch.save(model.state_dict(), 'best_model.pth')  # Save best model\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\n# Load the best model for evaluation\nmodel.load_state_dict(torch.load('best_model.pth'))\n\n# Define the test folder path\ntest_dir = \"/kaggle/working/plates/plates/test\"\n\n# Define transformations for test data (no augmentation, only normalization)\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Load test images manually, skipping non-image files\ntest_images = []\ntest_ids = []\n\nfor img_name in os.listdir(test_dir):\n    img_path = os.path.join(test_dir, img_name)\n    \n    # Skip non-image files like .DS_Store\n    if img_name.startswith('.') or not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n        continue\n    \n    image = Image.open(img_path).convert(\"RGB\")  # Ensure 3 channels (RGB)\n    image = transform_test(image)  # Apply transformations\n    test_images.append(image)\n    test_ids.append(img_name.split('.')[0])  # Extract ID from filename\n\nprint(f\"Loaded {len(test_images)} test images.\")\n\n# Convert list to tensor\ntest_images = torch.stack(test_images)\n\n# Create a TensorDataset for test data\ntest_dataset = TensorDataset(test_images)\n\n# Create a DataLoader for batching (batch_size 8)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n# Set model to evaluation mode\nmodel.eval()\n\n# List to store predictions\npredictions = []\n\n# Inference loop for test dataset\nwith torch.no_grad():  # No need to track gradients during inference\n    for images in test_loader:\n        images = images[0].to(device)  # Images are wrapped in a tuple, so access images[0]\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        predictions.extend(preds.cpu().numpy())\n\n# Convert predictions to 'clean' or 'dirty' labels\npredicted_labels = ['clean' if pred == 0 else 'dirty' for pred in predictions]\n\n# Create the submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': test_ids,\n    'label': predicted_labels\n})\n\n# Save the predictions to CSV\nsubmission_df.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"Submission saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T18:22:48.567554Z","iopub.execute_input":"2024-12-04T18:22:48.568346Z","iopub.status.idle":"2024-12-04T18:23:05.674812Z","shell.execute_reply.started":"2024-12-04T18:22:48.568310Z","shell.execute_reply":"2024-12-04T18:23:05.673938Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 179MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50, Loss: 0.7051941951115926\nEpoch 2/50, Loss: 0.33659713466962177\nEpoch 3/50, Loss: 0.15761467814445496\nEpoch 4/50, Loss: 0.0846199095249176\nEpoch 5/50, Loss: 0.07047259310881297\nEpoch 6/50, Loss: 0.03993051995833715\nEpoch 7/50, Loss: 0.026158249005675316\nEpoch 8/50, Loss: 0.010716338797161976\nEpoch 9/50, Loss: 0.004927833486969273\nEpoch 10/50, Loss: 0.003358266782015562\nEpoch 11/50, Loss: 0.0051191871364911394\nEpoch 12/50, Loss: 0.027025277881572645\nEpoch 13/50, Loss: 0.004840530299892028\nEpoch 14/50, Loss: 0.007950032750765482\nEpoch 15/50, Loss: 0.00821786723099649\nEarly stopping triggered!\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/3282673488.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Loaded 744 test images.\nSubmission saved!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"################  ABOVE CODE GAVE BEST SCORE SO FAR - 0.64 ############","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T18:24:15.688161Z","iopub.execute_input":"2024-12-04T18:24:15.688508Z","iopub.status.idle":"2024-12-04T18:24:15.692470Z","shell.execute_reply.started":"2024-12-04T18:24:15.688474Z","shell.execute_reply":"2024-12-04T18:24:15.691559Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets, transforms, models\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nimport pandas as pd\n\n# Define the correct path to the train folder\ntrain_dir = \"/kaggle/working/plates/plates/train\"\n\n# Define transformations for training (with more advanced augmentations)\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),                    # Resize all images to 224x224\n    transforms.RandomHorizontalFlip(),                # Random horizontal flip\n    transforms.RandomRotation(30),                    # Random rotation between -30 and 30 degrees\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random resized crop with scaling\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Color jitter\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for pretrained models\n])\n\n# Load the training dataset\ntrain_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n\n# Create DataLoader for the training dataset\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# Load ResNet50 model (deeper network)\nmodel = models.resnet50(pretrained=True)\n\n# Unfreeze some layers to fine-tune\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Modify the final fully connected layer to match the number of classes\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)  # Output size = 2 (clean, dirty)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Optimizer for fine-tuning all layers\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# Learning rate scheduler for cosine annealing\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.00001)\n\n# Loss function\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Training loop\nepochs = 50\nbest_loss = float('inf')\npatience = 5\ncounter = 0\n\nfor epoch in range(epochs):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    # Adjust learning rate dynamically\n    scheduler.step()\n\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n    \n    # Early stopping logic\n    if running_loss < best_loss:\n        best_loss = running_loss\n        counter = 0  # Reset counter\n        torch.save(model.state_dict(), 'best_model.pth')  # Save best model\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\n# Load the best model for evaluation\nmodel.load_state_dict(torch.load('best_model.pth'))\n\n# Define the test folder path\ntest_dir = \"/kaggle/working/plates/plates/test\"\n\n# Define transformations for test data (no augmentation, only normalization)\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pretrained model normalization\n])\n\n# Load test images manually, skipping non-image files\ntest_images = []\ntest_ids = []\n\nfor img_name in os.listdir(test_dir):\n    img_path = os.path.join(test_dir, img_name)\n    \n    # Skip non-image files like .DS_Store\n    if img_name.startswith('.') or not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n        continue\n    \n    image = Image.open(img_path).convert(\"RGB\")  # Ensure 3 channels (RGB)\n    image = transform_test(image)  # Apply transformations\n    test_images.append(image)\n    test_ids.append(img_name.split('.')[0])  # Extract ID from filename\n\nprint(f\"Loaded {len(test_images)} test images.\")\n\n# Convert list to tensor\ntest_images = torch.stack(test_images)\n\n# Create a TensorDataset for test data\ntest_dataset = TensorDataset(test_images)\n\n# Create a DataLoader for batching (batch_size 8)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n# Set model to evaluation mode\nmodel.eval()\n\n# List to store predictions\npredictions = []\n\n# Inference loop for test dataset\nwith torch.no_grad():  # No need to track gradients during inference\n    for images in test_loader:\n        images = images[0].to(device)  # Images are wrapped in a tuple, so access images[0]\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        predictions.extend(preds.cpu().numpy())\n\n# Convert predictions to 'clean' or 'dirty' labels\npredicted_labels = ['clean' if pred == 0 else 'dirty' for pred in predictions]\n\n# Create the submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': test_ids,\n    'label': predicted_labels\n})\n\n# Save the predictions to CSV\nsubmission_df.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"Submission saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T18:26:03.607450Z","iopub.execute_input":"2024-12-04T18:26:03.607779Z","iopub.status.idle":"2024-12-04T18:26:33.108973Z","shell.execute_reply.started":"2024-12-04T18:26:03.607748Z","shell.execute_reply":"2024-12-04T18:26:33.107942Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/70, Loss: 0.7128284970919291\nEpoch 2/70, Loss: 0.45205386479695636\nEpoch 3/70, Loss: 0.2572313646475474\nEpoch 4/70, Loss: 0.20672126611073813\nEpoch 5/70, Loss: 0.10521839807430904\nEpoch 6/70, Loss: 0.06839549044768016\nEpoch 7/70, Loss: 0.16460398336251578\nEpoch 8/70, Loss: 0.059296103194355965\nEpoch 9/70, Loss: 0.02875618450343609\nEpoch 10/70, Loss: 0.057815043876568474\nEpoch 11/70, Loss: 0.013257823574046293\nEpoch 12/70, Loss: 0.01698884212722381\nEpoch 13/70, Loss: 0.09265145411094029\nEpoch 14/70, Loss: 0.1184906146178643\nEpoch 15/70, Loss: 0.026426262377450865\nEpoch 16/70, Loss: 0.06074249201143781\nEpoch 17/70, Loss: 0.007826247562964758\nEpoch 18/70, Loss: 0.014785638079047203\nEpoch 19/70, Loss: 0.05922134670739373\nEpoch 20/70, Loss: 0.07654039810101192\nEpoch 21/70, Loss: 0.18316089113553366\nEpoch 22/70, Loss: 0.38185271869103116\nEpoch 23/70, Loss: 0.1325375083833933\nEpoch 24/70, Loss: 0.31198861077427864\nEpoch 25/70, Loss: 0.023040825501084328\nEpoch 26/70, Loss: 0.03438739602764448\nEpoch 27/70, Loss: 0.025554596136013668\nEarly stopping triggered!\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/1271430863.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Loaded 744 test images.\nSubmission saved!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"###################   BEST SCORE SO FAR - 0.65 ################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T18:27:27.003289Z","iopub.execute_input":"2024-12-04T18:27:27.003629Z","iopub.status.idle":"2024-12-04T18:27:27.007723Z","shell.execute_reply.started":"2024-12-04T18:27:27.003599Z","shell.execute_reply":"2024-12-04T18:27:27.006749Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets, transforms, models\nimport torch.optim as optim\nfrom PIL import Image\nimport os\nimport pandas as pd\nimport numpy as np\n\n# Define the correct path to the train folder\ntrain_dir = \"/kaggle/working/plates/plates/train\"\n\n# Define transformations for training with more advanced augmentations\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),                    \n    transforms.RandomHorizontalFlip(),                \n    transforms.RandomRotation(30),                    \n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  \n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  \n    transforms.RandomAffine(30),  # Random affine transformations\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n])\n\n# Define Cutout augmentation\nfrom PIL import Image\nimport numpy as np\n\nclass Cutout(object):\n    def __init__(self, length):\n        self.length = length\n    \n    def __call__(self, img):\n        # Convert image to NumPy array\n        img = np.array(img)\n        \n        # Check if image has enough size for cutout\n        h, w, _ = img.shape\n        if h > self.length and w > self.length:\n            top = np.random.randint(0, h - self.length)\n            left = np.random.randint(0, w - self.length)\n            img[top:top + self.length, left:left + self.length, :] = 0\n\n        # Convert back to PIL Image before returning\n        return Image.fromarray(img)\n\n# Ensure transformation chain is correct\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize all images\n    transforms.ToTensor(),         # Convert to Tensor (correct data type)\n    Cutout(length=50)              # Apply Cutout\n])\n\n\ncutout_transform = Cutout(50)  # Random cutout of 50px\ntransform_train.transforms.append(cutout_transform)\n\n# Load the training dataset\ntrain_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)\n\n# Create DataLoader for the training dataset\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# Use EfficientNet (efficient architecture) instead of ResNet50\nfrom torchvision import models\n\nmodel = models.efficientnet_b0(pretrained=True)\n\n# Unfreeze all layers for fine-tuning\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Modify the final fully connected layer to match the number of classes\nmodel.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)  # Output size = 2 (clean, dirty)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Optimizer for fine-tuning all layers with L2 regularization (weight decay)\noptimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n\n# Learning rate scheduler for cosine annealing\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.00001)\n\n# Loss function (use Focal Loss if classes are imbalanced)\nclass FocalLoss(torch.nn.Module):\n    def __init__(self, alpha=0.25, gamma=2, num_classes=2):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.num_classes = num_classes\n        self.ce_loss = torch.nn.CrossEntropyLoss(reduction='none')\n\n    def forward(self, inputs, targets):\n        ce_loss = self.ce_loss(inputs, targets)\n        pt = torch.exp(-ce_loss)  # Probability of correct class\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        return focal_loss.mean()\n\ncriterion = FocalLoss()\n\n# Training loop\nepochs = 50\nbest_loss = float('inf')\npatience = 5\ncounter = 0\n\nfor epoch in range(epochs):\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n    \n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    # Adjust learning rate dynamically\n    scheduler.step()\n\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n    \n    # Early stopping logic\n    if running_loss < best_loss:\n        best_loss = running_loss\n        counter = 0  # Reset counter\n        torch.save(model.state_dict(), 'best_model.pth')  # Save best model\n    else:\n        counter += 1\n        if counter >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\n# Load the best model for evaluation\nmodel.load_state_dict(torch.load('best_model.pth'))\n\n# Define the test folder path\ntest_dir = \"/kaggle/working/plates/plates/test\"\n\n# Define transformations for test data (no augmentation, only normalization)\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load test images manually\ntest_images = []\ntest_ids = []\n\nfor img_name in os.listdir(test_dir):\n    img_path = os.path.join(test_dir, img_name)\n    \n    # Skip non-image files like .DS_Store\n    if img_name.startswith('.') or not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n        continue\n    \n    image = Image.open(img_path).convert(\"RGB\")  # Ensure 3 channels (RGB)\n    image = transform_test(image)  # Apply transformations\n    test_images.append(image)\n    test_ids.append(img_name.split('.')[0])  # Extract ID from filename\n\nprint(f\"Loaded {len(test_images)} test images.\")\n\n# Convert list to tensor\ntest_images = torch.stack(test_images)\n\n# Create a TensorDataset for test data\ntest_dataset = TensorDataset(test_images)\n\n# Create a DataLoader for batching (batch_size 8)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n# Set model to evaluation mode\nmodel.eval()\n\n# List to store predictions\npredictions = []\n\n# Inference loop for test dataset\nwith torch.no_grad():  # No need to track gradients during inference\n    for images in test_loader:\n        images = images[0].to(device)  # Images are wrapped in a tuple, so access images[0]\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        predictions.extend(preds.cpu().numpy())\n\n# Convert predictions to 'clean' or 'dirty' labels\npredicted_labels = ['clean' if pred == 0 else 'dirty' for pred in predictions]\n\n# Create the submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': test_ids,\n    'label': predicted_labels\n})\n\n# Save the predictions to CSV\nsubmission_df.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"Submission saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T18:29:18.098341Z","iopub.execute_input":"2024-12-04T18:29:18.098646Z","iopub.status.idle":"2024-12-04T18:29:18.386018Z","shell.execute_reply.started":"2024-12-04T18:29:18.098621Z","shell.execute_reply":"2024-12-04T18:29:18.384765Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3315\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3315\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","\u001b[0;31mKeyError\u001b[0m: ((1, 1, 224), '<f4')","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[1;32m    111\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    114\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","Cell \u001b[0;32mIn[17], line 45\u001b[0m, in \u001b[0;36mCutout.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     42\u001b[0m     img[top:top \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength, left:left \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength, :] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Convert back to PIL Image before returning\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3319\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3317\u001b[0m         typekey_shape, typestr \u001b[38;5;241m=\u001b[39m typekey\n\u001b[1;32m   3318\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypekey_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypestr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3319\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3321\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n","\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 224), <f4"],"ename":"TypeError","evalue":"Cannot handle this data type: (1, 1, 224), <f4","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}